{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Wesley_Tao\\6.Software\\Anaconda\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation) \n",
    "lemma = WordNetLemmatizer()\n",
    "def clean(doc):\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RT', '@marcobonzanini', ':', 'just', 'an', 'example', '!', ':D', 'http://example.com', '#NLP']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    " \n",
    "emoticons_str = r\"\"\"\n",
    "    (?:\n",
    "        [:=;] # Eyes\n",
    "        [oO\\-]? # Nose (optional)\n",
    "        [D\\)\\]\\(\\]/\\\\OpP] # Mouth\n",
    "    )\"\"\"\n",
    " \n",
    "regex_str = [\n",
    "    emoticons_str,\n",
    "    r'<[^>]+>', # HTML tags\n",
    "    r'(?:@[\\w_]+)', # @-mentions\n",
    "    r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", # hash-tags\n",
    "    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs\n",
    " \n",
    "    r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', # numbers\n",
    "    r\"(?:[a-z][a-z'\\-_]+[a-z])\", # words with - and '\n",
    "    r'(?:[\\w_]+)', # other words\n",
    "    r'(?:\\S)' # anything else\n",
    "]\n",
    "    \n",
    "tokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE)\n",
    "emoticon_re = re.compile(r'^'+emoticons_str+'$', re.VERBOSE | re.IGNORECASE)\n",
    " \n",
    "def tokenize(s):\n",
    "    return tokens_re.findall(s)\n",
    " \n",
    "def preprocess(s, lowercase=False):\n",
    "    tokens = tokenize(s)\n",
    "#     print(tokens_re.search(tokens[0]))\n",
    "    if lowercase:\n",
    "        tokens =  [token if emoticon_re.search(token)  else token.lower() for token in tokens]\n",
    "    return tokens\n",
    " \n",
    "tweet = 'RT @marcobonzanini: just an example! :D http://example.com #NLP'\n",
    "print(preprocess(tweet))\n",
    "# ['RT', '@marcobonzanini', ':', 'just', 'a n', 'example', '!', ':D', 'http://example.com', '#NLP']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "def remove_stop_words(tweet):\n",
    "    out=tweet\n",
    "    if(tweet):\n",
    "        p=re.compile(r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+') # URLs\n",
    "        p2=re.compile(r'(?:@[\\w_]+)')\n",
    "        punctuation = list(string.punctuation)\n",
    "        stop = stopwords.words('english') + punctuation + ['via',\"\\r\",r'<[^>]+>',\"RT\",\"…\",\"’\",\"—\",\"u\"]+[r'(?:@[\\w_]+)']\n",
    "    #     print(stop)\n",
    "        terms_stop = [term for term in preprocess(tweet) if p.match(term)==None and term not in stop and p2.match(term)==None]\n",
    "    #     terms_stop = [term for term in preprocess(tweet) if term not in stop]\n",
    "        out=\" \".join(terms_stop)\n",
    "    return(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# US news "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "myfile=[]\n",
    "for file in os.listdir(\"../data/News/US\"):\n",
    "    myfile.append(\"../data/News/US/\"+file)\n",
    "article_data_us=[]\n",
    "for thisfile in myfile:\n",
    "    with open(thisfile, 'r') as f:\n",
    "        thisdoc=f.read()\n",
    "        article_data_us.append(thisdoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(article_data_us)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "article_data_us=list(map(remove_stop_words,article_data_us))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Trump trade war China tells US defend national interests 24 March 2018 Share Facebook Share Twitter Share Messenger Share Email Share Image copyrightREUTERS Image caption President Trump announced plans impose tariffs 60 bn Chinese products China warned US defend interests trade Chinese state media says US President Donald Trump backed tariffs Chinese goods The comments came phone call China's vice-premier Liu He US Treasury Secretary Steven Mnuchin Mr Trump announced plans impose tariffs 60 bn ￡ 42.5 bn Chinese goods accusing China intellectual property theft The move rattled markets stoked fears trade war Mr Liu Chinese President Xi Jinping's top economic adviser told Mr Mnuchin Beijing ready defend national interests hoped sides remain rational work together China's official Xinhua news agency reported During telephone conversation thought highest-level contact two governments since Mr Trump announced tariffs Thursday Mr Liu also accused US violating international trade rules following investigation Chinese intellectual property practices What trade war would affect Trump Tariffs 60 bn Chinese goods What could China US trade war Amid tensions trade World Trade Organization Director General Roberto Azev êdo warned new trade barriers would jeopardise global economy Mr Trump however said US move raise tariffs China already beginning get results Many countries negotiating fair trade deals us president said Friday Following Mr Trump's move China said planning retaliate set proposed tariffs worth 3 bn including tariffs groceries aluminium scrap Beijing warned US afraid trade war said hopes avoid one continued dialogue\""
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_data_us[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def topic(list_document,num_topics):\n",
    "    \"\"\"list_document is a list of document\"\"\"\n",
    "    num_topics=num_topics\n",
    "    doc_clean = [clean(doc).split() for doc in list_document] \n",
    "    dictionary = corpora.Dictionary(doc_clean)\n",
    "    doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "    Lda = gensim.models.ldamodel.LdaModel\n",
    "    ldamodel = Lda(doc_term_matrix, num_topics=num_topics, id2word = dictionary, passes=50)\n",
    "    mylist=[]\n",
    "    for topic in range(num_topics):\n",
    "        termlist=ldamodel.print_topics(num_topics=num_topics, num_words=10)[topic][1].split(\"+\")\n",
    "        mylist.append(list(map(lambda term:term.split(\"*\"),termlist)))\n",
    "    pandas_list=[]\n",
    "    for topic_index in range(num_topics):\n",
    "        this_topic=pd.DataFrame(mylist[topic_index],columns=[\"prob\",\"key_words\"])\n",
    "        this_topic[\"topic\"]=topic_index\n",
    "        pandas_list.append(this_topic)\n",
    "    LDA_result=pd.concat(pandas_list)\n",
    "    return(LDA_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prob</th>\n",
       "      <th>key_words</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.020</td>\n",
       "      <td>\"china\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.016</td>\n",
       "      <td>\"trade\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.010</td>\n",
       "      <td>\"growth\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.009</td>\n",
       "      <td>\"trump\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.008</td>\n",
       "      <td>\"economy\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.007</td>\n",
       "      <td>\"year\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.007</td>\n",
       "      <td>\"said\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.006</td>\n",
       "      <td>\"state\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.006</td>\n",
       "      <td>\"war\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.006</td>\n",
       "      <td>\"2018\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.013</td>\n",
       "      <td>\"china\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.010</td>\n",
       "      <td>\"trade\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.009</td>\n",
       "      <td>\"state\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.008</td>\n",
       "      <td>\"united\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.008</td>\n",
       "      <td>\"5\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.007</td>\n",
       "      <td>\"trump\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.007</td>\n",
       "      <td>\"g\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.007</td>\n",
       "      <td>\"u\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.007</td>\n",
       "      <td>\"war\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.007</td>\n",
       "      <td>\"farmer\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.031</td>\n",
       "      <td>\"u\"</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.030</td>\n",
       "      <td>\"china\"</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.024</td>\n",
       "      <td>\"trade\"</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.015</td>\n",
       "      <td>\"tariff\"</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.013</td>\n",
       "      <td>\"said\"</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.012</td>\n",
       "      <td>\"chinese\"</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.012</td>\n",
       "      <td>\"war\"</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.011</td>\n",
       "      <td>\"trump\"</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.008</td>\n",
       "      <td>\"share\"</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.007</td>\n",
       "      <td>\"import\"</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     prob   key_words  topic\n",
       "0   0.020    \"china\"       0\n",
       "1   0.016    \"trade\"       0\n",
       "2   0.010   \"growth\"       0\n",
       "3   0.009    \"trump\"       0\n",
       "4   0.008  \"economy\"       0\n",
       "5   0.007     \"year\"       0\n",
       "6   0.007     \"said\"       0\n",
       "7   0.006    \"state\"       0\n",
       "8   0.006      \"war\"       0\n",
       "9   0.006      \"2018\"      0\n",
       "0   0.013    \"china\"       1\n",
       "1   0.010    \"trade\"       1\n",
       "2   0.009    \"state\"       1\n",
       "3   0.008   \"united\"       1\n",
       "4   0.008        \"5\"       1\n",
       "5   0.007    \"trump\"       1\n",
       "6   0.007        \"g\"       1\n",
       "7   0.007        \"u\"       1\n",
       "8   0.007      \"war\"       1\n",
       "9   0.007    \"farmer\"      1\n",
       "0   0.031        \"u\"       2\n",
       "1   0.030    \"china\"       2\n",
       "2   0.024    \"trade\"       2\n",
       "3   0.015   \"tariff\"       2\n",
       "4   0.013     \"said\"       2\n",
       "5   0.012  \"chinese\"       2\n",
       "6   0.012      \"war\"       2\n",
       "7   0.011    \"trump\"       2\n",
       "8   0.008    \"share\"       2\n",
       "9   0.007    \"import\"      2"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic(article_data_us,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# China"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "myfile=[]\n",
    "for file in os.listdir(\"../data/News/China\"):\n",
    "    myfile.append(\"../data/News/China/\"+file)\n",
    "article_data_china=[]\n",
    "for thisfile in myfile:\n",
    "    with open(thisfile, 'r') as f:\n",
    "        thisdoc=f.read()\n",
    "        article_data_china.append(thisdoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(article_data_china)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "article_data_china=list(map(remove_stop_words,article_data_china))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Trade protectionism never win US competitiveness experts By Zhang Niansheng Wu Lejun Hu Zexi Zhang Mengxu People's Daily 09 42, April 09, 2018 The short-sighted trade protectionism measures taken US helpless improve competitiveness global arena experts pointed Office US Trade Representative USTR Tuesday published proposed list Chinese goods subject additional 25 percent tariffs despite strong opposition China US business groups The proposed list covered approximately 1,300 products imported China based so-called Section 301 investigation alleged Chinese intellectual property technology transfer practices launched Trump administration August 2017 The restricted measures launched US mainly targeting technology industries aerospace information communication technology robotics machinery cited Made China 2025 strategy plan upgrade manufacturing sector excuse But experts found trade deficit US China blamed long-term discriminatory export controls China well strict control high-tech exports China The trade deficit China US would drop 24 34 percent Washington would liberalize export barriers China level applicable Brazil France said report released US-headquartered think tank Carnegie Endowment International Peace The deficit could narrowed US eases restrictions high-tech exports China based consultation said report adding move time satisfy China demands technology upgrade In global supply chain taken shape China occupies seat advanced manufacturing sectors hard find alternative short run said Yukon Huang senior research fellow Carnegie Endowment International Peace Against background US companies pay extra costs China-made products transferred expenses consumers explained Huang also former Country Director China World Bank adding US public final victim since employment daily life affected A 25 percent tariff imported Chinese information communication tech parts products could slow growth US output 332 billion decade US-based think tank Information Technology Innovation Foundation ITIF estimated recent report The US suffer less information communication technology investment lower creativity declined productivity weakened competitiveness result decision since tech products constitute key driver productivity growth commodities service industries report concluded There ground blame technology transfer along trade cooperation consensual “ We learning process together ” Jochem Heizmann chief executive Volkswagen China operations told New York Times commenting transferring technology China electric car industry “ That process much faster used things ” added The dilapidated China-US cooperation aerospace sector teach US lesson The country government departments objector bilateral cooperation fear collaboration may lead technology leakage result put country technology advantages especially military segment risks Joan Johnson-Freese professor National Security Affairs US-based Naval War College told People Daily But business circle expected closer cooperation China belief way learn China advanced technologies contribute mankind exploration outer space based sound model international cooperation professor added The conservative decision makers ultimately gained upper hand debate leaving narrow room bilateral cooperation field according Johnson-Freese China aerospace technology finally advanced rapid pace gone beyond expectation US especially breakthrough quantum communication technology “ The US hoped beef cooperation China already missed best time window ” said Johnson-Freese\""
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_data_china[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prob</th>\n",
       "      <th>key_words</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.030</td>\n",
       "      <td>\"china\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.022</td>\n",
       "      <td>\"trade\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.021</td>\n",
       "      <td>\"u\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.011</td>\n",
       "      <td>\"said\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.010</td>\n",
       "      <td>\"war\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.008</td>\n",
       "      <td>\"trump\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.008</td>\n",
       "      <td>\"chinese\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.008</td>\n",
       "      <td>\"”\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.008</td>\n",
       "      <td>\"“\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.008</td>\n",
       "      <td>\"2018\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.026</td>\n",
       "      <td>\"china\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.022</td>\n",
       "      <td>\"u\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.014</td>\n",
       "      <td>\"trade\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.010</td>\n",
       "      <td>\"chinese\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.008</td>\n",
       "      <td>\"said\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.008</td>\n",
       "      <td>\"war\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.007</td>\n",
       "      <td>\"trump\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.006</td>\n",
       "      <td>\"tariff\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.005</td>\n",
       "      <td>\"product\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.005</td>\n",
       "      <td>\"beijing\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.027</td>\n",
       "      <td>\"u\"</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.025</td>\n",
       "      <td>\"china\"</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.013</td>\n",
       "      <td>\"trade\"</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.011</td>\n",
       "      <td>\"said\"</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.009</td>\n",
       "      <td>\"tariff\"</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.007</td>\n",
       "      <td>\"soybean\"</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.006</td>\n",
       "      <td>\"chinese\"</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.006</td>\n",
       "      <td>\"percent\"</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.006</td>\n",
       "      <td>\"lng\"</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.005</td>\n",
       "      <td>\"would\"</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     prob   key_words  topic\n",
       "0   0.030    \"china\"       0\n",
       "1   0.022    \"trade\"       0\n",
       "2   0.021        \"u\"       0\n",
       "3   0.011     \"said\"       0\n",
       "4   0.010      \"war\"       0\n",
       "5   0.008    \"trump\"       0\n",
       "6   0.008  \"chinese\"       0\n",
       "7   0.008        \"”\"       0\n",
       "8   0.008        \"“\"       0\n",
       "9   0.008      \"2018\"      0\n",
       "0   0.026    \"china\"       1\n",
       "1   0.022        \"u\"       1\n",
       "2   0.014    \"trade\"       1\n",
       "3   0.010  \"chinese\"       1\n",
       "4   0.008     \"said\"       1\n",
       "5   0.008      \"war\"       1\n",
       "6   0.007    \"trump\"       1\n",
       "7   0.006   \"tariff\"       1\n",
       "8   0.005  \"product\"       1\n",
       "9   0.005   \"beijing\"      1\n",
       "0   0.027        \"u\"       2\n",
       "1   0.025    \"china\"       2\n",
       "2   0.013    \"trade\"       2\n",
       "3   0.011     \"said\"       2\n",
       "4   0.009   \"tariff\"       2\n",
       "5   0.007  \"soybean\"       2\n",
       "6   0.006  \"chinese\"       2\n",
       "7   0.006  \"percent\"       2\n",
       "8   0.006      \"lng\"       2\n",
       "9   0.005     \"would\"      2"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic(article_data_china,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FOX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open\n",
    "fh = open(\"/Users/jiangyiran/Desktop/Stat_2nd/BDA/Project/News/FOX.txt\")\n",
    "doc1 = fh.read()\n",
    "doc_complete = [doc1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_clean = [clean(doc).split() for doc in doc_complete] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Lda = gensim.models.ldamodel.LdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ldamodel = Lda(doc_term_matrix, num_topics=3, id2word = dictionary, passes=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.023*\"farmer\" + 0.016*\"trade\" + 0.016*\"china\" + 0.012*\"u\" + 0.012*\"trump\" + 0.010*\"said\" + 0.010*\"agriculture\" + 0.008*\"president\" + 0.008*\"fox\" + 0.008*\"many\"'), (1, '0.003*\"farmer\" + 0.003*\"china\" + 0.003*\"trade\" + 0.003*\"tech\" + 0.003*\"economic\" + 0.003*\"actively\" + 0.003*\"feel\" + 0.003*\"reentering\" + 0.003*\"operator\" + 0.003*\"carry\"'), (2, '0.003*\"farmer\" + 0.003*\"china\" + 0.003*\"trade\" + 0.003*\"trump\" + 0.003*\"“ambassador\" + 0.003*\"king\" + 0.003*\"kansa\" + 0.003*\"still\" + 0.003*\"advantage\" + 0.003*\"job\"')]\n"
     ]
    }
   ],
   "source": [
    "print(ldamodel.print_topics(num_topics=10, num_words=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### China Daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.031*\"china\" + 0.027*\"u\" + 0.023*\"said\" + 0.020*\"trade\" + 0.009*\"chinese\" + 0.009*\"tariff\" + 0.009*\"soybean\" + 0.007*\"world\" + 0.006*\"lng\" + 0.006*\"would\"'), (1, '0.001*\"china\" + 0.001*\"u\" + 0.001*\"said\" + 0.001*\"trade\" + 0.001*\"soybean\" + 0.001*\"tariff\" + 0.001*\"world\" + 0.001*\"chinese\" + 0.001*\"would\" + 0.001*\"farmer\"'), (2, '0.001*\"china\" + 0.001*\"u\" + 0.001*\"said\" + 0.001*\"trade\" + 0.001*\"chinese\" + 0.001*\"world\" + 0.001*\"tariff\" + 0.001*\"soybean\" + 0.001*\"year\" + 0.001*\"lng\"')]\n"
     ]
    }
   ],
   "source": [
    "fh = open(\"/Users/jiangyiran/Desktop/Stat_2nd/BDA/Project/News/ChinaDaily/10.txt\")\n",
    "doc1 = fh.read()\n",
    "doc_complete = [doc1]\n",
    "doc_clean = [clean(doc).split() for doc in doc_complete] \n",
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=3, id2word = dictionary, passes=50)\n",
    "print(ldamodel.print_topics(num_topics=10, num_words=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.001*\"trade\" + 0.001*\"china\" + 0.001*\"said\" + 0.001*\"u\" + 0.001*\"trump\" + 0.001*\"state\" + 0.001*\"tariff\" + 0.001*\"united\" + 0.001*\"chinese\" + 0.001*\"war\"'), (1, '0.001*\"china\" + 0.001*\"trade\" + 0.001*\"said\" + 0.001*\"u\" + 0.001*\"trump\" + 0.001*\"war\" + 0.001*\"state\" + 0.001*\"united\" + 0.001*\"tariff\" + 0.001*\"chinese\"'), (2, '0.022*\"china\" + 0.022*\"trade\" + 0.013*\"u\" + 0.013*\"said\" + 0.011*\"trump\" + 0.010*\"chinese\" + 0.009*\"state\" + 0.009*\"war\" + 0.009*\"tariff\" + 0.008*\"united\"')]\n"
     ]
    }
   ],
   "source": [
    "fh = open(\"/Users/jiangyiran/Desktop/Stat_2nd/BDA/Project/News/CNN/CNNALL.txt\")\n",
    "doc1 = fh.read()\n",
    "doc_complete = [doc1]\n",
    "doc_clean = [clean(doc).split() for doc in doc_complete] \n",
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=3, id2word = dictionary, passes=50)\n",
    "print(ldamodel.print_topics(num_topics=10, num_words=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.025*\"china\" + 0.018*\"u\" + 0.016*\"trade\" + 0.012*\"chinese\" + 0.009*\"said\" + 0.007*\"world\" + 0.007*\"people\" + 0.006*\"product\" + 0.006*\"measure\" + 0.006*\"international\"'), (1, '0.001*\"china\" + 0.001*\"u\" + 0.001*\"trade\" + 0.001*\"chinese\" + 0.001*\"world\" + 0.001*\"said\" + 0.001*\"people\" + 0.001*\"product\" + 0.001*\"war\" + 0.001*\"international\"'), (2, '0.001*\"china\" + 0.001*\"u\" + 0.001*\"trade\" + 0.001*\"said\" + 0.001*\"chinese\" + 0.001*\"people\" + 0.001*\"world\" + 0.001*\"china’s\" + 0.001*\"product\" + 0.001*\"international\"')]\n"
     ]
    }
   ],
   "source": [
    "fh = open(\"/Users/jiangyiran/Desktop/Stat_2nd/BDA/Project/News/Renmin/Renmin.txt\")\n",
    "doc1 = fh.read()\n",
    "doc_complete = [doc1]\n",
    "doc_clean = [clean(doc).split() for doc in doc_complete] \n",
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=3, id2word = dictionary, passes=50)\n",
    "print(ldamodel.print_topics(num_topics=10, num_words=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SCMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.025*\"china\" + 0.024*\"u\" + 0.017*\"chinese\" + 0.016*\"trade\" + 0.015*\"said\" + 0.013*\"2018\" + 0.009*\"canada\" + 0.009*\"photo\" + 0.008*\"tariff\" + 0.008*\"import\"'), (1, '0.003*\"u\" + 0.003*\"chinese\" + 0.003*\"china\" + 0.003*\"said\" + 0.003*\"trade\" + 0.003*\"official\" + 0.003*\"edition\" + 0.003*\"unilateral\" + 0.003*\"hope\" + 0.003*\"unfair\"'), (2, '0.003*\"china\" + 0.003*\"u\" + 0.003*\"chinese\" + 0.003*\"trade\" + 0.003*\"2018\" + 0.003*\"said\" + 0.003*\"photo\" + 0.003*\"apr\" + 0.003*\"war\" + 0.003*\"canada\"')]\n"
     ]
    }
   ],
   "source": [
    "fh = open(\"/Users/jiangyiran/Desktop/Stat_2nd/BDA/Project/News/SCMP/SCMP.txt\")\n",
    "doc1 = fh.read()\n",
    "doc_complete = [doc1]\n",
    "doc_clean = [clean(doc).split() for doc in doc_complete] \n",
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=3, id2word = dictionary, passes=50)\n",
    "print(ldamodel.print_topics(num_topics=10, num_words=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SCM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.002*\"u\" + 0.002*\"trade\" + 0.002*\"china\" + 0.002*\"bank\" + 0.002*\"company\" + 0.002*\"bloomberg\" + 0.002*\"three\" + 0.002*\"war\" + 0.002*\"2018\" + 0.002*\"tariff\"'), (1, '0.020*\"china\" + 0.020*\"trade\" + 0.018*\"u\" + 0.012*\"bloomberg\" + 0.011*\"bank\" + 0.010*\"company\" + 0.010*\"new\" + 0.008*\"would\" + 0.008*\"war\" + 0.008*\"chinese\"'), (2, '0.002*\"trade\" + 0.002*\"china\" + 0.002*\"u\" + 0.002*\"new\" + 0.002*\"bank\" + 0.002*\"bloomberg\" + 0.002*\"would\" + 0.002*\"chinese\" + 0.002*\"trump’s\" + 0.002*\"tariff\"')]\n"
     ]
    }
   ],
   "source": [
    "fh = open(\"/Users/jiangyiran/Desktop/Stat_2nd/BDA/Project/News/SOUTHCHINAMORNING.txt\")\n",
    "doc1 = fh.read()\n",
    "doc_complete = [doc1]\n",
    "doc_clean = [clean(doc).split() for doc in doc_complete] \n",
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=3, id2word = dictionary, passes=50)\n",
    "print(ldamodel.print_topics(num_topics=10, num_words=10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
