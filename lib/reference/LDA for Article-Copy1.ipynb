{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Wesley_Tao\\6.Software\\Anaconda\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation) \n",
    "lemma = WordNetLemmatizer()\n",
    "def clean(doc):\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myfile=[]\n",
    "for file in os.listdir(\"../data/News/US\"):\n",
    "    myfile.append(\"../data/News/US/\"+file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "article_data=[]\n",
    "for thisfile in myfile:\n",
    "    with open(thisfile, 'r') as f:\n",
    "        thisdoc=f.read()\n",
    "        article_data.append(thisdoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(article_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_clean = [clean(doc).split() for doc in article_data] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FOX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "myfile=[]\n",
    "for file in os.listdir(\"../data/News_Original/FOX\"):\n",
    "    myfile.append(\"../data/News_Original/FOX/\"+file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/News_Original/FOX/FOX.txt']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_complete=[]\n",
    "for thisfile in myfile:\n",
    "    with open(thisfile, 'r') as f:\n",
    "        thisdoc=f.read()\n",
    "        doc_complete.append(thisdoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    " \n",
    "emoticons_str = r\"\"\"\n",
    "    (?:\n",
    "        [:=;] # Eyes\n",
    "        [oO\\-]? # Nose (optional)\n",
    "        [D\\)\\]\\(\\]/\\\\OpP] # Mouth\n",
    "    )\"\"\"\n",
    " \n",
    "regex_str = [\n",
    "    emoticons_str,\n",
    "    r'<[^>]+>', # HTML tags\n",
    "    r'(?:@[\\w_]+)', # @-mentions\n",
    "    r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", # hash-tags\n",
    "    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs\n",
    " \n",
    "    r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', # numbers\n",
    "    r\"(?:[a-z][a-z'\\-_]+[a-z])\", # words with - and '\n",
    "    r'(?:[\\w_]+)', # other words\n",
    "    r'(?:\\S)' # anything else\n",
    "]\n",
    "    \n",
    "tokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE)\n",
    "emoticon_re = re.compile(r'^'+emoticons_str+'$', re.VERBOSE | re.IGNORECASE)\n",
    " \n",
    "def tokenize(s):\n",
    "    return tokens_re.findall(s)\n",
    " \n",
    "def preprocess(s, lowercase=False):\n",
    "    tokens = tokenize(s)\n",
    "#     print(tokens_re.search(tokens[0]))\n",
    "    if lowercase:\n",
    "        tokens =  [token if emoticon_re.search(token)  else token.lower() for token in tokens]\n",
    "    return tokens\n",
    " \n",
    "tweet = 'RT @marcobonzanini: just an example! :D http://example.com #NLP'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "def remove_stop_words(tweet):\n",
    "    out=tweet\n",
    "    if(tweet):\n",
    "        p=re.compile(r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+') # URLs\n",
    "        p2=re.compile(r'(?:@[\\w_]+)')\n",
    "        punctuation = list(string.punctuation)\n",
    "        stop = stopwords.words('english') + punctuation + ['via',\"\\r\",r'<[^>]+>',\"RT\",\"…\",\"’\",\"—\",\"u\"]+[r'(?:@[\\w_]+)']\n",
    "    #     print(stop)\n",
    "        terms_stop = [term for term in preprocess(tweet) if p.match(term)==None and term not in stop and p2.match(term)==None]\n",
    "    #     terms_stop = [term for term in preprocess(tweet) if term not in stop]\n",
    "        out=\" \".join(terms_stop)\n",
    "    return(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_clean = [remove_stop_words(doc).split() for doc in doc_complete] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Lda = gensim.models.ldamodel.LdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ldamodel = Lda(doc_term_matrix, num_topics=3, id2word = dictionary, passes=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.003*\"china\" + 0.003*\"farmer\" + 0.003*\"trade\" + 0.003*\"trump\" + 0.003*\"u\" + 0.003*\"agriculture\" + 0.003*\"said\" + 0.003*\"fox\" + 0.003*\"april\" + 0.003*\"house\"'), (1, '0.023*\"farmer\" + 0.016*\"trade\" + 0.016*\"china\" + 0.012*\"trump\" + 0.012*\"u\" + 0.010*\"agriculture\" + 0.010*\"said\" + 0.008*\"war\" + 0.008*\"fox\" + 0.008*\"house\"'), (2, '0.003*\"farmer\" + 0.003*\"trade\" + 0.003*\"u\" + 0.003*\"china\" + 0.003*\"said\" + 0.003*\"told\" + 0.003*\"trans\" + 0.003*\"brunt\" + 0.003*\"heavily\" + 0.003*\"oak\"')]\n"
     ]
    }
   ],
   "source": [
    "print(ldamodel.print_topics(num_topics=10, num_words=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def topic(list_document,num_topics,num_words):\n",
    "    \"\"\"list_document is a list of string , each element is a document\"\"\"\n",
    "    num_topics=num_topics\n",
    "    doc_clean = [remove_stop_words(doc).split() for doc in list_document] \n",
    "    dictionary = corpora.Dictionary(doc_clean)\n",
    "    doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "    Lda = gensim.models.ldamodel.LdaModel\n",
    "    ldamodel = Lda(doc_term_matrix, num_topics=num_topics, id2word = dictionary, passes=50)\n",
    "    mylist=[]\n",
    "    for topic in range(num_topics):\n",
    "        termlist=ldamodel.print_topics(num_topics=num_topics, num_words=num_words)[topic][1].split(\"+\")\n",
    "        mylist.append(list(map(lambda term:term.split(\"*\"),termlist)))\n",
    "    pandas_list=[]\n",
    "    for topic_index in range(num_topics):\n",
    "        this_topic=pd.DataFrame(mylist[topic_index],columns=[\"prob\",\"key_words\"])\n",
    "        this_topic[\"topic\"]=topic_index\n",
    "        pandas_list.append(this_topic)\n",
    "    LDA_result=pd.concat(pandas_list)\n",
    "    return(LDA_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prob</th>\n",
       "      <th>key_words</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.032</td>\n",
       "      <td>\"China\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.028</td>\n",
       "      <td>\"US\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.018</td>\n",
       "      <td>\"said\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.006</td>\n",
       "      <td>\"farmers\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.005</td>\n",
       "      <td>\"uncertainty\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.004</td>\n",
       "      <td>\"corn\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.020</td>\n",
       "      <td>\"said\"</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.017</td>\n",
       "      <td>\"trade\"</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.016</td>\n",
       "      <td>\"China\"</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     prob       key_words  topic\n",
       "0   0.032        \"China\"       0\n",
       "1   0.028           \"US\"       0\n",
       "2   0.018          \"said\"      0\n",
       "0   0.006      \"farmers\"       1\n",
       "1   0.005  \"uncertainty\"       1\n",
       "2   0.004          \"corn\"      1\n",
       "0   0.020         \"said\"       2\n",
       "1   0.017        \"trade\"       2\n",
       "2   0.016         \"China\"      2"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic(doc_complete,3,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### China Daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "media=\"ChinaDaily\"\n",
    "\n",
    "myfile=[]\n",
    "for file in os.listdir(\"../data/News_Original/\"+media):\n",
    "    myfile.append(\"../data/News_Original/\"+media+\"/\"+file)\n",
    "doc_complete=[]\n",
    "for thisfile in myfile:\n",
    "    with open(thisfile, 'r') as f:\n",
    "        thisdoc=f.read()\n",
    "        doc_complete.append(thisdoc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prob</th>\n",
       "      <th>key_words</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.029</td>\n",
       "      <td>\"US\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.026</td>\n",
       "      <td>\"China\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.018</td>\n",
       "      <td>\"said\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.016</td>\n",
       "      <td>\"trade\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.010</td>\n",
       "      <td>\"tariff\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.027</td>\n",
       "      <td>\"said\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.022</td>\n",
       "      <td>\"trade\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.014</td>\n",
       "      <td>\"The\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.014</td>\n",
       "      <td>\"China\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.012</td>\n",
       "      <td>\"Graham\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.020</td>\n",
       "      <td>\"China\"</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.019</td>\n",
       "      <td>\"said\"</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.014</td>\n",
       "      <td>\"US\"</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.011</td>\n",
       "      <td>\"Cui\"</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.008</td>\n",
       "      <td>\"Chinese\"</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.029</td>\n",
       "      <td>\"LNG\"</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.026</td>\n",
       "      <td>\"US\"</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.023</td>\n",
       "      <td>\"China\"</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.016</td>\n",
       "      <td>\"oil\"</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.010</td>\n",
       "      <td>\"imports\"</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.017</td>\n",
       "      <td>\"China\"</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.014</td>\n",
       "      <td>\"said\"</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.011</td>\n",
       "      <td>\"Chinese\"</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.011</td>\n",
       "      <td>\"“\"</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.010</td>\n",
       "      <td>\"trade\"</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     prob   key_words  topic\n",
       "0   0.029       \"US\"       0\n",
       "1   0.026    \"China\"       0\n",
       "2   0.018     \"said\"       0\n",
       "3   0.016    \"trade\"       0\n",
       "4   0.010    \"tariff\"      0\n",
       "0   0.027     \"said\"       1\n",
       "1   0.022    \"trade\"       1\n",
       "2   0.014      \"The\"       1\n",
       "3   0.014    \"China\"       1\n",
       "4   0.012    \"Graham\"      1\n",
       "0   0.020    \"China\"       2\n",
       "1   0.019     \"said\"       2\n",
       "2   0.014       \"US\"       2\n",
       "3   0.011      \"Cui\"       2\n",
       "4   0.008   \"Chinese\"      2\n",
       "0   0.029      \"LNG\"       3\n",
       "1   0.026       \"US\"       3\n",
       "2   0.023    \"China\"       3\n",
       "3   0.016      \"oil\"       3\n",
       "4   0.010   \"imports\"      3\n",
       "0   0.017    \"China\"       4\n",
       "1   0.014     \"said\"       4\n",
       "2   0.011  \"Chinese\"       4\n",
       "3   0.011        \"“\"       4\n",
       "4   0.010     \"trade\"      4"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic(doc_complete,5,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.031*\"china\" + 0.027*\"u\" + 0.023*\"said\" + 0.020*\"trade\" + 0.009*\"chinese\" + 0.009*\"tariff\" + 0.009*\"soybean\" + 0.007*\"world\" + 0.006*\"lng\" + 0.006*\"would\"'), (1, '0.001*\"china\" + 0.001*\"u\" + 0.001*\"said\" + 0.001*\"trade\" + 0.001*\"soybean\" + 0.001*\"tariff\" + 0.001*\"world\" + 0.001*\"chinese\" + 0.001*\"would\" + 0.001*\"farmer\"'), (2, '0.001*\"china\" + 0.001*\"u\" + 0.001*\"said\" + 0.001*\"trade\" + 0.001*\"chinese\" + 0.001*\"world\" + 0.001*\"tariff\" + 0.001*\"soybean\" + 0.001*\"year\" + 0.001*\"lng\"')]\n"
     ]
    }
   ],
   "source": [
    "fh = open(\"/Users/jiangyiran/Desktop/Stat_2nd/BDA/Project/News/ChinaDaily/10.txt\")\n",
    "doc1 = fh.read()\n",
    "doc_complete = [doc1]\n",
    "doc_clean = [clean(doc).split() for doc in doc_complete] \n",
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=3, id2word = dictionary, passes=50)\n",
    "print(ldamodel.print_topics(num_topics=10, num_words=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prob</th>\n",
       "      <th>key_words</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.016</td>\n",
       "      <td>\"trade\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.014</td>\n",
       "      <td>\"G\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.013</td>\n",
       "      <td>\"5\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.011</td>\n",
       "      <td>\"Trump\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.011</td>\n",
       "      <td>\"China\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.014</td>\n",
       "      <td>\"China\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.014</td>\n",
       "      <td>\"trade\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.013</td>\n",
       "      <td>\"US\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.010</td>\n",
       "      <td>\"Chinese\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.009</td>\n",
       "      <td>\"would\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.012</td>\n",
       "      <td>\"The\"</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.009</td>\n",
       "      <td>\"Thursday\"</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.009</td>\n",
       "      <td>\"data\"</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.007</td>\n",
       "      <td>\"US\"</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.007</td>\n",
       "      <td>\"trade\"</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001</td>\n",
       "      <td>\"trade\"</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001</td>\n",
       "      <td>\"The\"</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001</td>\n",
       "      <td>\"China\"</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001</td>\n",
       "      <td>\"said\"</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001</td>\n",
       "      <td>\"Chinese\"</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.020</td>\n",
       "      <td>\"China\"</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.019</td>\n",
       "      <td>\"trade\"</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.013</td>\n",
       "      <td>\"said\"</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.012</td>\n",
       "      <td>\"US\"</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.012</td>\n",
       "      <td>\"The\"</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     prob    key_words  topic\n",
       "0   0.016     \"trade\"       0\n",
       "1   0.014         \"G\"       0\n",
       "2   0.013         \"5\"       0\n",
       "3   0.011     \"Trump\"       0\n",
       "4   0.011      \"China\"      0\n",
       "0   0.014     \"China\"       1\n",
       "1   0.014     \"trade\"       1\n",
       "2   0.013        \"US\"       1\n",
       "3   0.010   \"Chinese\"       1\n",
       "4   0.009      \"would\"      1\n",
       "0   0.012       \"The\"       2\n",
       "1   0.009  \"Thursday\"       2\n",
       "2   0.009      \"data\"       2\n",
       "3   0.007        \"US\"       2\n",
       "4   0.007      \"trade\"      2\n",
       "0   0.001     \"trade\"       3\n",
       "1   0.001       \"The\"       3\n",
       "2   0.001     \"China\"       3\n",
       "3   0.001      \"said\"       3\n",
       "4   0.001    \"Chinese\"      3\n",
       "0   0.020     \"China\"       4\n",
       "1   0.019     \"trade\"       4\n",
       "2   0.013      \"said\"       4\n",
       "3   0.012        \"US\"       4\n",
       "4   0.012        \"The\"      4"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "media=\"CNN\"\n",
    "\n",
    "myfile=[]\n",
    "for file in os.listdir(\"../data/News_Original/\"+media):\n",
    "    myfile.append(\"../data/News_Original/\"+media+\"/\"+file)\n",
    "doc_complete=[]\n",
    "for thisfile in myfile:\n",
    "    with open(thisfile, 'r') as f:\n",
    "        thisdoc=f.read()\n",
    "        doc_complete.append(thisdoc)\n",
    "topic(doc_complete,5,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.001*\"trade\" + 0.001*\"china\" + 0.001*\"said\" + 0.001*\"u\" + 0.001*\"trump\" + 0.001*\"state\" + 0.001*\"tariff\" + 0.001*\"united\" + 0.001*\"chinese\" + 0.001*\"war\"'), (1, '0.001*\"china\" + 0.001*\"trade\" + 0.001*\"said\" + 0.001*\"u\" + 0.001*\"trump\" + 0.001*\"war\" + 0.001*\"state\" + 0.001*\"united\" + 0.001*\"tariff\" + 0.001*\"chinese\"'), (2, '0.022*\"china\" + 0.022*\"trade\" + 0.013*\"u\" + 0.013*\"said\" + 0.011*\"trump\" + 0.010*\"chinese\" + 0.009*\"state\" + 0.009*\"war\" + 0.009*\"tariff\" + 0.008*\"united\"')]\n"
     ]
    }
   ],
   "source": [
    "fh = open(\"/Users/jiangyiran/Desktop/Stat_2nd/BDA/Project/News/CNN/CNNALL.txt\")\n",
    "doc1 = fh.read()\n",
    "doc_complete = [doc1]\n",
    "doc_clean = [clean(doc).split() for doc in doc_complete] \n",
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=3, id2word = dictionary, passes=50)\n",
    "print(ldamodel.print_topics(num_topics=10, num_words=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.025*\"china\" + 0.018*\"u\" + 0.016*\"trade\" + 0.012*\"chinese\" + 0.009*\"said\" + 0.007*\"world\" + 0.007*\"people\" + 0.006*\"product\" + 0.006*\"measure\" + 0.006*\"international\"'), (1, '0.001*\"china\" + 0.001*\"u\" + 0.001*\"trade\" + 0.001*\"chinese\" + 0.001*\"world\" + 0.001*\"said\" + 0.001*\"people\" + 0.001*\"product\" + 0.001*\"war\" + 0.001*\"international\"'), (2, '0.001*\"china\" + 0.001*\"u\" + 0.001*\"trade\" + 0.001*\"said\" + 0.001*\"chinese\" + 0.001*\"people\" + 0.001*\"world\" + 0.001*\"china’s\" + 0.001*\"product\" + 0.001*\"international\"')]\n"
     ]
    }
   ],
   "source": [
    "fh = open(\"/Users/jiangyiran/Desktop/Stat_2nd/BDA/Project/News/Renmin/Renmin.txt\")\n",
    "doc1 = fh.read()\n",
    "doc_complete = [doc1]\n",
    "doc_clean = [clean(doc).split() for doc in doc_complete] \n",
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=3, id2word = dictionary, passes=50)\n",
    "print(ldamodel.print_topics(num_topics=10, num_words=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prob</th>\n",
       "      <th>key_words</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.033</td>\n",
       "      <td>\"US\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.026</td>\n",
       "      <td>\"China\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.015</td>\n",
       "      <td>\"trade\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.013</td>\n",
       "      <td>\"said\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.010</td>\n",
       "      <td>\"The\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.031</td>\n",
       "      <td>\"China\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.024</td>\n",
       "      <td>\"trade\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.009</td>\n",
       "      <td>\"said\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.009</td>\n",
       "      <td>\"The\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.008</td>\n",
       "      <td>\"new\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.015</td>\n",
       "      <td>\"China\"</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.014</td>\n",
       "      <td>\"China's\"</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.010</td>\n",
       "      <td>\"trade\"</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.010</td>\n",
       "      <td>\"US\"</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.009</td>\n",
       "      <td>\"opening-up\"</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.018</td>\n",
       "      <td>\"Chinese\"</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.016</td>\n",
       "      <td>\"We\"</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.012</td>\n",
       "      <td>\"people\"</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.011</td>\n",
       "      <td>\"world\"</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.010</td>\n",
       "      <td>\"future\"</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.027</td>\n",
       "      <td>\"US\"</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.018</td>\n",
       "      <td>\"WTO\"</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.014</td>\n",
       "      <td>\"trade\"</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.012</td>\n",
       "      <td>\"The\"</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.011</td>\n",
       "      <td>\"301\"</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.023</td>\n",
       "      <td>\"China\"</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.013</td>\n",
       "      <td>\"Taiwan\"</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.012</td>\n",
       "      <td>\"US\"</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.011</td>\n",
       "      <td>\"Chinese\"</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.009</td>\n",
       "      <td>\"American\"</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     prob     key_words  topic\n",
       "0   0.033         \"US\"       0\n",
       "1   0.026      \"China\"       0\n",
       "2   0.015      \"trade\"       0\n",
       "3   0.013       \"said\"       0\n",
       "4   0.010         \"The\"      0\n",
       "0   0.031      \"China\"       1\n",
       "1   0.024      \"trade\"       1\n",
       "2   0.009       \"said\"       1\n",
       "3   0.009        \"The\"       1\n",
       "4   0.008         \"new\"      1\n",
       "0   0.015      \"China\"       2\n",
       "1   0.014    \"China's\"       2\n",
       "2   0.010      \"trade\"       2\n",
       "3   0.010         \"US\"       2\n",
       "4   0.009  \"opening-up\"      2\n",
       "0   0.018    \"Chinese\"       3\n",
       "1   0.016         \"We\"       3\n",
       "2   0.012     \"people\"       3\n",
       "3   0.011      \"world\"       3\n",
       "4   0.010      \"future\"      3\n",
       "0   0.027         \"US\"       4\n",
       "1   0.018        \"WTO\"       4\n",
       "2   0.014      \"trade\"       4\n",
       "3   0.012        \"The\"       4\n",
       "4   0.011         \"301\"      4\n",
       "0   0.023      \"China\"       5\n",
       "1   0.013     \"Taiwan\"       5\n",
       "2   0.012         \"US\"       5\n",
       "3   0.011    \"Chinese\"       5\n",
       "4   0.009    \"American\"      5"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "media=\"Renmin\"\n",
    "\n",
    "myfile=[]\n",
    "for file in os.listdir(\"../data/News_Original/\"+media):\n",
    "    myfile.append(\"../data/News_Original/\"+media+\"/\"+file)\n",
    "doc_complete=[]\n",
    "for thisfile in myfile:\n",
    "    with open(thisfile, 'r') as f:\n",
    "        thisdoc=f.read()\n",
    "        doc_complete.append(thisdoc)\n",
    "topic(doc_complete,6,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SCMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prob</th>\n",
       "      <th>key_words</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000</td>\n",
       "      <td>\"China\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000</td>\n",
       "      <td>\"US\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000</td>\n",
       "      <td>\"trade\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000</td>\n",
       "      <td>\"”\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000</td>\n",
       "      <td>\"Trump\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.027</td>\n",
       "      <td>\"China\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.024</td>\n",
       "      <td>\"US\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.020</td>\n",
       "      <td>\"trade\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.013</td>\n",
       "      <td>\"Trump\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.011</td>\n",
       "      <td>\"war\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.028</td>\n",
       "      <td>\"China\"</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.024</td>\n",
       "      <td>\"US\"</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.013</td>\n",
       "      <td>\"trade\"</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.009</td>\n",
       "      <td>\"Chinese\"</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.009</td>\n",
       "      <td>\"war\"</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.012</td>\n",
       "      <td>\"Canada\"</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.012</td>\n",
       "      <td>\"US\"</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.011</td>\n",
       "      <td>\"Chinese\"</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.008</td>\n",
       "      <td>\"said\"</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.007</td>\n",
       "      <td>\"steel\"</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000</td>\n",
       "      <td>\"China\"</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000</td>\n",
       "      <td>\"trade\"</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000</td>\n",
       "      <td>\"US\"</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000</td>\n",
       "      <td>\"“\"</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000</td>\n",
       "      <td>\"war\"</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     prob   key_words  topic\n",
       "0   0.000    \"China\"       0\n",
       "1   0.000       \"US\"       0\n",
       "2   0.000    \"trade\"       0\n",
       "3   0.000        \"”\"       0\n",
       "4   0.000     \"Trump\"      0\n",
       "0   0.027    \"China\"       1\n",
       "1   0.024       \"US\"       1\n",
       "2   0.020    \"trade\"       1\n",
       "3   0.013    \"Trump\"       1\n",
       "4   0.011       \"war\"      1\n",
       "0   0.028    \"China\"       2\n",
       "1   0.024       \"US\"       2\n",
       "2   0.013    \"trade\"       2\n",
       "3   0.009  \"Chinese\"       2\n",
       "4   0.009       \"war\"      2\n",
       "0   0.012   \"Canada\"       3\n",
       "1   0.012       \"US\"       3\n",
       "2   0.011  \"Chinese\"       3\n",
       "3   0.008     \"said\"       3\n",
       "4   0.007     \"steel\"      3\n",
       "0   0.000    \"China\"       4\n",
       "1   0.000    \"trade\"       4\n",
       "2   0.000       \"US\"       4\n",
       "3   0.000        \"“\"       4\n",
       "4   0.000       \"war\"      4"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "media=\"South China Morning Post\"\n",
    "\n",
    "myfile=[]\n",
    "for file in os.listdir(\"../data/News_Original/\"+media):\n",
    "    myfile.append(\"../data/News_Original/\"+media+\"/\"+file)\n",
    "doc_complete=[]\n",
    "for thisfile in myfile:\n",
    "    with open(thisfile, 'r') as f:\n",
    "        thisdoc=f.read()\n",
    "        doc_complete.append(thisdoc)\n",
    "topic(doc_complete,5,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.025*\"china\" + 0.024*\"u\" + 0.017*\"chinese\" + 0.016*\"trade\" + 0.015*\"said\" + 0.013*\"2018\" + 0.009*\"canada\" + 0.009*\"photo\" + 0.008*\"tariff\" + 0.008*\"import\"'), (1, '0.003*\"u\" + 0.003*\"chinese\" + 0.003*\"china\" + 0.003*\"said\" + 0.003*\"trade\" + 0.003*\"official\" + 0.003*\"edition\" + 0.003*\"unilateral\" + 0.003*\"hope\" + 0.003*\"unfair\"'), (2, '0.003*\"china\" + 0.003*\"u\" + 0.003*\"chinese\" + 0.003*\"trade\" + 0.003*\"2018\" + 0.003*\"said\" + 0.003*\"photo\" + 0.003*\"apr\" + 0.003*\"war\" + 0.003*\"canada\"')]\n"
     ]
    }
   ],
   "source": [
    "fh = open(\"/Users/jiangyiran/Desktop/Stat_2nd/BDA/Project/News/SCMP/SCMP.txt\")\n",
    "doc1 = fh.read()\n",
    "doc_complete = [doc1]\n",
    "doc_clean = [clean(doc).split() for doc in doc_complete] \n",
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=3, id2word = dictionary, passes=50)\n",
    "print(ldamodel.print_topics(num_topics=10, num_words=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SCM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.002*\"u\" + 0.002*\"trade\" + 0.002*\"china\" + 0.002*\"bank\" + 0.002*\"company\" + 0.002*\"bloomberg\" + 0.002*\"three\" + 0.002*\"war\" + 0.002*\"2018\" + 0.002*\"tariff\"'), (1, '0.020*\"china\" + 0.020*\"trade\" + 0.018*\"u\" + 0.012*\"bloomberg\" + 0.011*\"bank\" + 0.010*\"company\" + 0.010*\"new\" + 0.008*\"would\" + 0.008*\"war\" + 0.008*\"chinese\"'), (2, '0.002*\"trade\" + 0.002*\"china\" + 0.002*\"u\" + 0.002*\"new\" + 0.002*\"bank\" + 0.002*\"bloomberg\" + 0.002*\"would\" + 0.002*\"chinese\" + 0.002*\"trump’s\" + 0.002*\"tariff\"')]\n"
     ]
    }
   ],
   "source": [
    "fh = open(\"/Users/jiangyiran/Desktop/Stat_2nd/BDA/Project/News/SOUTHCHINAMORNING.txt\")\n",
    "doc1 = fh.read()\n",
    "doc_complete = [doc1]\n",
    "doc_clean = [clean(doc).split() for doc in doc_complete] \n",
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=3, id2word = dictionary, passes=50)\n",
    "print(ldamodel.print_topics(num_topics=10, num_words=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, unicode_literals, division\n",
    "from bz2 import BZ2File\n",
    "import time\n",
    "import plac\n",
    "import ujson\n",
    "from spacy.matcher import PhraseMatcher\n",
    "import spacy\n",
    "import time \n",
    "import gc\n",
    "from spacy import displacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp=spacy.load(\"en\")\n",
    "class FileProcessor():\n",
    "   \n",
    "    \n",
    "    ''' we need pandas,spacy to function'''\n",
    "    def __init__(self,name,text):\n",
    "        self.docname=name\n",
    "        self.doc=nlp(text)\n",
    "        self.wordnum=len(self.doc)\n",
    "        self.trivnum=len(self.doc)-len(self.SelectQualified())\n",
    "        \n",
    "    def GetTokenDF(self):\n",
    "        \"\"\"we want to get the whole dataframe with token details with the help of pandas \"\"\"\n",
    "        tokendf=pd.DataFrame(columns=[\"text\",\"lemma\",\"pos\",\"tag\",\"dep\",\"is_stop\"])\n",
    "        for token in self.doc:\n",
    "            tokendf.loc[token.text]=[token.text, token.lemma_ ,token.pos_, token.tag_, token.dep_, token.is_stop]\n",
    "        return tokendf\n",
    "    \n",
    "    def SelectQualified(self):\n",
    "        \"\"\"this is to select qualified words display in dataframe\"\"\"\n",
    "        tokendf=self.GetTokenDF()\n",
    "        sampleset=[\"PROPN\",\"NOUN\",\"VERB\",\"ADJ\",\"ADV\"] # pick only noun verb adj and adv words\n",
    "        Qtoken=tokendf.loc[(tokendf.pos.isin(sampleset))& (tokendf.is_stop == False)] # is_stop  means the trivial words \"of the a\" \n",
    "        return Qtoken\n",
    "    \n",
    "    def GetEntityDF(self):\n",
    "        \"\"\"get entity data frame excluding cardinal and ordinal\"\"\"\n",
    "        entitydf=pd.DataFrame(columns=\"text,start_char,end_char,label\".split(\",\"))\n",
    "        for ent in self.doc.ents:\n",
    "            entitydf.loc[ent.text]=[ent.text, ent.start_char, ent.end_char, ent.label_]\n",
    "            entitydf=entitydf.loc[(entitydf.label!=\"CARDINAL\") & (entitydf.label!=\"ORDINAL\")] # cardinal and ordinal are not what we want\n",
    "        return entitydf\n",
    "    \n",
    "    def GetWordChunkDF(self):\n",
    "        \"\"\"get word chunk  dataframe\"\"\"\n",
    "        samplechunk=pd.DataFrame(columns=\"text, root.text, root.dep_,head.text\".split(\",\"))\n",
    "        for chunk in self.doc.noun_chunks:\n",
    "            samplechunk.loc[chunk.text]=[chunk.text, chunk.root.text, chunk.root.dep_,chunk.root.head.text]\n",
    "        return samplechunk\n",
    "       \n",
    "    def GetAvgDepth(self):\n",
    "        \n",
    "        def getdepth(token):\n",
    "             # if right child exist search right child\n",
    "            if not list(token.children):\n",
    "                return 0\n",
    "            else:\n",
    "                leftmax= 0 if not list(token.lefts) else max([getdepth(i)  for i in token.lefts])\n",
    "            # if left child exist search leftchild\n",
    "                rightmax=0  if not list(token.rights) else max([getdepth(i)  for i in token.rights])\n",
    "                return max(leftmax,rightmax)+1\n",
    "            # if not child return 0\n",
    "            # return the max number\n",
    "        count=0\n",
    "        mydepth=dict()\n",
    "        for token in self.doc:\n",
    "            if token.dep_==\"ROOT\":\n",
    "                # iterating each dep and get the depth\n",
    "                count=count+1\n",
    "                mydepth[count]=getdepth(token)\n",
    "        return(sum(mydepth.values())/count)\n",
    "    def GetAvgSenlen(self):\n",
    "        sentences = [sent.string.strip() for sent in self.doc.sents]\n",
    "        return len(self.doc)/len(sentences)\n",
    "    \n",
    "    def Getall(self):\n",
    "        return pd.DataFrame([[self.wordnum,self.trivnum,len(self.GetEntityDF()),len(self.GetWordChunkDF()),self.GetAvgDepth(),self.GetAvgSenlen()]],\\\n",
    "                            columns=[\"Total words\",\"Trivial words\",\"entities\",\"wordchunk\",\"AvgDepth\",\"AvgLen\"],index=[self.docname]) \n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prossessor=FileProcessor(\"sample\",remove_stop_words(doc_complete[0])) # build a file processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total words</th>\n",
       "      <th>Trivial words</th>\n",
       "      <th>entities</th>\n",
       "      <th>wordchunk</th>\n",
       "      <th>AvgDepth</th>\n",
       "      <th>AvgLen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sample</th>\n",
       "      <td>475</td>\n",
       "      <td>227</td>\n",
       "      <td>50</td>\n",
       "      <td>94</td>\n",
       "      <td>5.466667</td>\n",
       "      <td>31.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Total words  Trivial words  entities  wordchunk  AvgDepth     AvgLen\n",
       "sample          475            227        50         94  5.466667  31.666667"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prossessor.Getall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>start_char</th>\n",
       "      <th>end_char</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>US</th>\n",
       "      <td>US</td>\n",
       "      <td>3112</td>\n",
       "      <td>3114</td>\n",
       "      <td>GPE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Zhao Huanxin</th>\n",
       "      <td>Zhao Huanxin</td>\n",
       "      <td>83</td>\n",
       "      <td>95</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>China Daily Updated</th>\n",
       "      <td>China Daily Updated</td>\n",
       "      <td>96</td>\n",
       "      <td>115</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018</th>\n",
       "      <td>2018</td>\n",
       "      <td>116</td>\n",
       "      <td>120</td>\n",
       "      <td>DATE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18 08</th>\n",
       "      <td>18 08</td>\n",
       "      <td>124</td>\n",
       "      <td>129</td>\n",
       "      <td>DATE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>46</td>\n",
       "      <td>130</td>\n",
       "      <td>132</td>\n",
       "      <td>DATE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Donald Trump</th>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>233</td>\n",
       "      <td>245</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chinese</th>\n",
       "      <td>Chinese</td>\n",
       "      <td>576</td>\n",
       "      <td>583</td>\n",
       "      <td>NORP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>April 5</th>\n",
       "      <td>April 5</td>\n",
       "      <td>465</td>\n",
       "      <td>472</td>\n",
       "      <td>DATE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trump</th>\n",
       "      <td>Trump</td>\n",
       "      <td>3092</td>\n",
       "      <td>3097</td>\n",
       "      <td>NORP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>two days earlier</th>\n",
       "      <td>two days earlier</td>\n",
       "      <td>590</td>\n",
       "      <td>606</td>\n",
       "      <td>DATE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>China</th>\n",
       "      <td>China</td>\n",
       "      <td>3157</td>\n",
       "      <td>3162</td>\n",
       "      <td>GPE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25 percent</th>\n",
       "      <td>25 percent</td>\n",
       "      <td>635</td>\n",
       "      <td>645</td>\n",
       "      <td>PERCENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eight-year</th>\n",
       "      <td>eight-year</td>\n",
       "      <td>784</td>\n",
       "      <td>794</td>\n",
       "      <td>DATE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trump put</th>\n",
       "      <td>Trump put</td>\n",
       "      <td>802</td>\n",
       "      <td>811</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>last Monday</th>\n",
       "      <td>last Monday</td>\n",
       "      <td>812</td>\n",
       "      <td>823</td>\n",
       "      <td>DATE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pat Roberts</th>\n",
       "      <td>Pat Roberts</td>\n",
       "      <td>839</td>\n",
       "      <td>850</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PBS</th>\n",
       "      <td>PBS</td>\n",
       "      <td>1278</td>\n",
       "      <td>1281</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NewsHour Thursday</th>\n",
       "      <td>NewsHour Thursday</td>\n",
       "      <td>860</td>\n",
       "      <td>877</td>\n",
       "      <td>WORK_OF_ART</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6.7 percent</th>\n",
       "      <td>6.7 percent</td>\n",
       "      <td>948</td>\n",
       "      <td>959</td>\n",
       "      <td>PERCENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59.5 billion year</th>\n",
       "      <td>59.5 billion year</td>\n",
       "      <td>960</td>\n",
       "      <td>977</td>\n",
       "      <td>DATE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>since 2006</th>\n",
       "      <td>since 2006</td>\n",
       "      <td>985</td>\n",
       "      <td>995</td>\n",
       "      <td>DATE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>US Department Agriculture</th>\n",
       "      <td>US Department Agriculture</td>\n",
       "      <td>1007</td>\n",
       "      <td>1032</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Roberts</th>\n",
       "      <td>Roberts</td>\n",
       "      <td>1041</td>\n",
       "      <td>1048</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Senate Committee Agriculture Nutrition Forestry</th>\n",
       "      <td>Senate Committee Agriculture Nutrition Forestry</td>\n",
       "      <td>1063</td>\n",
       "      <td>1110</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Midwest</th>\n",
       "      <td>Midwest</td>\n",
       "      <td>1130</td>\n",
       "      <td>1137</td>\n",
       "      <td>LOC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trump trade</th>\n",
       "      <td>Trump trade</td>\n",
       "      <td>1152</td>\n",
       "      <td>1163</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Thursday</th>\n",
       "      <td>Thursday</td>\n",
       "      <td>2990</td>\n",
       "      <td>2998</td>\n",
       "      <td>DATE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>proposal least floated billions dollars</th>\n",
       "      <td>proposal least floated billions dollars</td>\n",
       "      <td>1185</td>\n",
       "      <td>1224</td>\n",
       "      <td>MONEY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Republican</th>\n",
       "      <td>Republican</td>\n",
       "      <td>1362</td>\n",
       "      <td>1372</td>\n",
       "      <td>NORP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kansas</th>\n",
       "      <td>Kansas</td>\n",
       "      <td>1266</td>\n",
       "      <td>1272</td>\n",
       "      <td>GPE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ben Sasse Nebraska</th>\n",
       "      <td>Ben Sasse Nebraska</td>\n",
       "      <td>1381</td>\n",
       "      <td>1399</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>statement day earlier</th>\n",
       "      <td>statement day earlier</td>\n",
       "      <td>1407</td>\n",
       "      <td>1428</td>\n",
       "      <td>DATE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>billions dollars</th>\n",
       "      <td>billions dollars</td>\n",
       "      <td>1456</td>\n",
       "      <td>1472</td>\n",
       "      <td>MONEY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Saturday</th>\n",
       "      <td>Saturday</td>\n",
       "      <td>1564</td>\n",
       "      <td>1572</td>\n",
       "      <td>DATE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>morning</th>\n",
       "      <td>morning</td>\n",
       "      <td>1573</td>\n",
       "      <td>1580</td>\n",
       "      <td>TIME</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sasse</th>\n",
       "      <td>Sasse</td>\n",
       "      <td>1625</td>\n",
       "      <td>1630</td>\n",
       "      <td>PRODUCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Iowa</th>\n",
       "      <td>Iowa</td>\n",
       "      <td>1681</td>\n",
       "      <td>1685</td>\n",
       "      <td>GPE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kim Reynolds</th>\n",
       "      <td>Kim Reynolds</td>\n",
       "      <td>1695</td>\n",
       "      <td>1707</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Capitol Hill</th>\n",
       "      <td>Capitol Hill</td>\n",
       "      <td>1829</td>\n",
       "      <td>1841</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wednesday</th>\n",
       "      <td>Wednesday</td>\n",
       "      <td>1842</td>\n",
       "      <td>1851</td>\n",
       "      <td>DATE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>American Soybean Association</th>\n",
       "      <td>American Soybean Association</td>\n",
       "      <td>1881</td>\n",
       "      <td>1909</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Last year</th>\n",
       "      <td>Last year</td>\n",
       "      <td>2081</td>\n",
       "      <td>2090</td>\n",
       "      <td>DATE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32.85 million metric tonnes</th>\n",
       "      <td>32.85 million metric tonnes</td>\n",
       "      <td>2102</td>\n",
       "      <td>2129</td>\n",
       "      <td>QUANTITY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62 percent</th>\n",
       "      <td>62 percent</td>\n",
       "      <td>2157</td>\n",
       "      <td>2167</td>\n",
       "      <td>PERCENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trump's</th>\n",
       "      <td>Trump's</td>\n",
       "      <td>2495</td>\n",
       "      <td>2502</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Washington</th>\n",
       "      <td>Washington</td>\n",
       "      <td>2606</td>\n",
       "      <td>2616</td>\n",
       "      <td>GPE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>John Heisdorffer</th>\n",
       "      <td>John Heisdorffer</td>\n",
       "      <td>2814</td>\n",
       "      <td>2830</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ASA</th>\n",
       "      <td>ASA</td>\n",
       "      <td>2841</td>\n",
       "      <td>2844</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>China Heisdorffer</th>\n",
       "      <td>China Heisdorffer</td>\n",
       "      <td>2945</td>\n",
       "      <td>2962</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                            text  \\\n",
       "US                                                                                            US   \n",
       "Zhao Huanxin                                                                        Zhao Huanxin   \n",
       "China Daily Updated                                                          China Daily Updated   \n",
       "2018                                                                                        2018   \n",
       "18 08                                                                                      18 08   \n",
       "46                                                                                            46   \n",
       "Donald Trump                                                                        Donald Trump   \n",
       "Chinese                                                                                  Chinese   \n",
       "April 5                                                                                  April 5   \n",
       "Trump                                                                                      Trump   \n",
       "two days earlier                                                                two days earlier   \n",
       "China                                                                                      China   \n",
       "25 percent                                                                            25 percent   \n",
       "eight-year                                                                            eight-year   \n",
       "Trump put                                                                              Trump put   \n",
       "last Monday                                                                          last Monday   \n",
       "Pat Roberts                                                                          Pat Roberts   \n",
       "PBS                                                                                          PBS   \n",
       "NewsHour Thursday                                                              NewsHour Thursday   \n",
       "6.7 percent                                                                          6.7 percent   \n",
       "59.5 billion year                                                              59.5 billion year   \n",
       "since 2006                                                                            since 2006   \n",
       "US Department Agriculture                                              US Department Agriculture   \n",
       "Roberts                                                                                  Roberts   \n",
       "Senate Committee Agriculture Nutrition Forestry  Senate Committee Agriculture Nutrition Forestry   \n",
       "Midwest                                                                                  Midwest   \n",
       "Trump trade                                                                          Trump trade   \n",
       "Thursday                                                                                Thursday   \n",
       "proposal least floated billions dollars                  proposal least floated billions dollars   \n",
       "Republican                                                                            Republican   \n",
       "Kansas                                                                                    Kansas   \n",
       "Ben Sasse Nebraska                                                            Ben Sasse Nebraska   \n",
       "statement day earlier                                                      statement day earlier   \n",
       "billions dollars                                                                billions dollars   \n",
       "Saturday                                                                                Saturday   \n",
       "morning                                                                                  morning   \n",
       "Sasse                                                                                      Sasse   \n",
       "Iowa                                                                                        Iowa   \n",
       "Kim Reynolds                                                                        Kim Reynolds   \n",
       "Capitol Hill                                                                        Capitol Hill   \n",
       "Wednesday                                                                              Wednesday   \n",
       "American Soybean Association                                        American Soybean Association   \n",
       "Last year                                                                              Last year   \n",
       "32.85 million metric tonnes                                          32.85 million metric tonnes   \n",
       "62 percent                                                                            62 percent   \n",
       "Trump's                                                                                  Trump's   \n",
       "Washington                                                                            Washington   \n",
       "John Heisdorffer                                                                John Heisdorffer   \n",
       "ASA                                                                                          ASA   \n",
       "China Heisdorffer                                                              China Heisdorffer   \n",
       "\n",
       "                                                start_char end_char  \\\n",
       "US                                                    3112     3114   \n",
       "Zhao Huanxin                                            83       95   \n",
       "China Daily Updated                                     96      115   \n",
       "2018                                                   116      120   \n",
       "18 08                                                  124      129   \n",
       "46                                                     130      132   \n",
       "Donald Trump                                           233      245   \n",
       "Chinese                                                576      583   \n",
       "April 5                                                465      472   \n",
       "Trump                                                 3092     3097   \n",
       "two days earlier                                       590      606   \n",
       "China                                                 3157     3162   \n",
       "25 percent                                             635      645   \n",
       "eight-year                                             784      794   \n",
       "Trump put                                              802      811   \n",
       "last Monday                                            812      823   \n",
       "Pat Roberts                                            839      850   \n",
       "PBS                                                   1278     1281   \n",
       "NewsHour Thursday                                      860      877   \n",
       "6.7 percent                                            948      959   \n",
       "59.5 billion year                                      960      977   \n",
       "since 2006                                             985      995   \n",
       "US Department Agriculture                             1007     1032   \n",
       "Roberts                                               1041     1048   \n",
       "Senate Committee Agriculture Nutrition Forestry       1063     1110   \n",
       "Midwest                                               1130     1137   \n",
       "Trump trade                                           1152     1163   \n",
       "Thursday                                              2990     2998   \n",
       "proposal least floated billions dollars               1185     1224   \n",
       "Republican                                            1362     1372   \n",
       "Kansas                                                1266     1272   \n",
       "Ben Sasse Nebraska                                    1381     1399   \n",
       "statement day earlier                                 1407     1428   \n",
       "billions dollars                                      1456     1472   \n",
       "Saturday                                              1564     1572   \n",
       "morning                                               1573     1580   \n",
       "Sasse                                                 1625     1630   \n",
       "Iowa                                                  1681     1685   \n",
       "Kim Reynolds                                          1695     1707   \n",
       "Capitol Hill                                          1829     1841   \n",
       "Wednesday                                             1842     1851   \n",
       "American Soybean Association                          1881     1909   \n",
       "Last year                                             2081     2090   \n",
       "32.85 million metric tonnes                           2102     2129   \n",
       "62 percent                                            2157     2167   \n",
       "Trump's                                               2495     2502   \n",
       "Washington                                            2606     2616   \n",
       "John Heisdorffer                                      2814     2830   \n",
       "ASA                                                   2841     2844   \n",
       "China Heisdorffer                                     2945     2962   \n",
       "\n",
       "                                                       label  \n",
       "US                                                       GPE  \n",
       "Zhao Huanxin                                          PERSON  \n",
       "China Daily Updated                                      ORG  \n",
       "2018                                                    DATE  \n",
       "18 08                                                   DATE  \n",
       "46                                                      DATE  \n",
       "Donald Trump                                          PERSON  \n",
       "Chinese                                                 NORP  \n",
       "April 5                                                 DATE  \n",
       "Trump                                                   NORP  \n",
       "two days earlier                                        DATE  \n",
       "China                                                    GPE  \n",
       "25 percent                                           PERCENT  \n",
       "eight-year                                              DATE  \n",
       "Trump put                                                ORG  \n",
       "last Monday                                             DATE  \n",
       "Pat Roberts                                           PERSON  \n",
       "PBS                                                      ORG  \n",
       "NewsHour Thursday                                WORK_OF_ART  \n",
       "6.7 percent                                          PERCENT  \n",
       "59.5 billion year                                       DATE  \n",
       "since 2006                                              DATE  \n",
       "US Department Agriculture                                ORG  \n",
       "Roberts                                               PERSON  \n",
       "Senate Committee Agriculture Nutrition Forestry          ORG  \n",
       "Midwest                                                  LOC  \n",
       "Trump trade                                              ORG  \n",
       "Thursday                                                DATE  \n",
       "proposal least floated billions dollars                MONEY  \n",
       "Republican                                              NORP  \n",
       "Kansas                                                   GPE  \n",
       "Ben Sasse Nebraska                                    PERSON  \n",
       "statement day earlier                                   DATE  \n",
       "billions dollars                                       MONEY  \n",
       "Saturday                                                DATE  \n",
       "morning                                                 TIME  \n",
       "Sasse                                                PRODUCT  \n",
       "Iowa                                                     GPE  \n",
       "Kim Reynolds                                          PERSON  \n",
       "Capitol Hill                                             ORG  \n",
       "Wednesday                                               DATE  \n",
       "American Soybean Association                             ORG  \n",
       "Last year                                               DATE  \n",
       "32.85 million metric tonnes                         QUANTITY  \n",
       "62 percent                                           PERCENT  \n",
       "Trump's                                                  ORG  \n",
       "Washington                                               GPE  \n",
       "John Heisdorffer                                      PERSON  \n",
       "ASA                                                      ORG  \n",
       "China Heisdorffer                                     PERSON  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prossessor.GetEntityDF()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
